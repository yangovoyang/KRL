def sort_by_head(param_list):
    for i in range(len(param_list)-1):  # 0, n-2
        for j in range(len(param_list)-i-1):  # i n-2-i
            if (param_list[j][0] > param_list[j+1][0]) or (param_list[j][0] == param_list[j+1][0] and param_list[j][1] > param_list[j+1][1]) or (param_list[j][0] == param_list[j+1][0] and param_list[j][1] == param_list[j+1][1] and param_list[j][2] > param_list[j+1][2]):
                param_list[j], param_list[j + 1] = param_list[j + 1], param_list[j]


def sort_by_relation(param_list):
    for i in range(len(param_list)-1):  # 0, n-2
        for j in range(len(param_list)-i-1):  # i n-2-i
            if (param_list[j][1] > param_list[j+1][1]) or (param_list[j][1] == param_list[j+1][1] and param_list[j][0] > param_list[j+1][0]) or (param_list[j][1] == param_list[j+1][1] and param_list[j][0] == param_list[j+1][0] and param_list[j][2] > param_list[j+1][2]):
                param_list[j], param_list[j + 1] = param_list[j + 1], param_list[j]


def sort_by_tail(param_list):
    for i in range(len(param_list)-1):  # 0, n-2
        for j in range(len(param_list)-i-1):  # i n-2-i
            if (param_list[j][2] > param_list[j+1][2]) or (param_list[j][2] == param_list[j+1][2] and param_list[j][0] > param_list[j+1][0]) or (param_list[j][2] == param_list[j+1][2] and param_list[j][0] == param_list[j+1][0] and param_list[j][1] > param_list[j+1][1]):
                param_list[j], param_list[j + 1] = param_list[j + 1], param_list[j]


if __name__ == '__main__':
    # parameters
    dataset_path = "benchmarks/heart/"  # in_path
    test_link_prediction = True
    test_triple_classification = True
    train_times = 1000
    n_batches = 100
    learning_rate = 0.001
    margin = 1
    unif_bern = 0
    entity_neg_rate = 1
    relation_neg_rate = 0
    optimizer_method = "SGD"
    # model store
    model_export_file = "/res/model.vec.tf"
    # store model's parameters
    model_parameters_file = "/res/embedding.json"

    # prepare for train and test
    print('The toolkit is importing datasets.')

    with open(dataset_path + 'relation2id.txt', 'r', encoding='utf8') as file:
        print('The total of relations is', int(file.readline()))

    with open(dataset_path + 'entity2id.txt', 'r', encoding='utf8') as file:
        print('The total of entities is', int(file.readline()))

    train_list = []
    with open(dataset_path + 'train2id.txt', 'r', encoding='utf8') as file:
        train_total = int(file.readline())
        for line in file.readlines():
            temp_list = line.strip().split()
            for index in range(len(temp_list)):
                temp_list[index] = int(temp_list[index])
            train_list.append(temp_list)

    sort_by_head(train_list)

    # 去重，并构建以head,relation,tail为先序的排列列表
    train_head = [0]*train_total
    train_tail = [0]*train_total
    train_relation = [0]*train_total
    frequent_entity = [0]*train_total  # 记录各个实体出现的次数
    frequent_relation = [0]*train_total  # 记录各个关系出现的次数
    train_head[0] = train_list[0]
    train_tail.append(train_list[0])
    train_relation.append(train_list[0])
    frequent_entity[train_list[0][0]] += 1
    frequent_entity[train_list[0][2]] += 1
    frequent_relation[train_list[0][0]] += 1

    index = 1  # 不计重复的索引
    for i in range(1, train_total):  # 1, train_total-1
        if (train_list[i][0]!=train_list[i-1][0]) or (train_list[i][1]!=train_list[i-1][0]) or (train_list[i][2]!=train_list[i-1][2]):
            train_head[index] = train_tail[index] = train_relation[index] = train_list[index] = train_list[i]
            index += 1
            frequent_entity[train_list[i][0]] += 1
            frequent_entity[train_list[i][2]] += 1
            frequent_relation[train_list[i][1]] += 1

    sort_by_head(train_head)
    sort_by_tail(train_tail)
    sort_by_relation(train_relation)
    print('The total of train triples is', index)


    
